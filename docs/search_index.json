[
["index.html", "User Guide for the Eddie Server Clusters Overview", " User Guide for the Eddie Server Clusters Junfan &amp; GitHub 2022-05-18 Overview The University provides free resources and support for research involving high performance computing (HPC), including multiple server clusters for storing and analyzing data. Find out more information at https://www.wiki.ed.ac.uk/display/ResearchServices/Eddie. This guide will help you access and use the UCSB server clusters effectively. Use cases for HPC include: big data parallel computing lengthy computation times restricted-use data For a broader introduction to HPC with clusters, see the HPC Carpentry’s lessons at https://hpc-carpentry.github.io/, or work through the exercises Please acknowledge the Eddie in publications and presentations if you are using the clusters in your research. "],
["accessing-the-server-clusters.html", "1 Accessing the server clusters 1.1 On campus 1.2 Off campus", " 1 Accessing the server clusters To access the server clusters, you must first request a user account. The request form can be found at http://csc.cnsi.ucsb.edu/acct. The job type selection should probably be Serial or Parallel, but select the option that best describes your needs. In the job type description you should describe any relevant information related to user groups, access to specific folders, or special software usage. If you need Stata, then you should request to be added to the econ group, which holds the license to use Stata. The system to be used should be Pod and Knot, which are the main Linux-based server clusters. Pod is the newest cluster and offers greater capabilities, but Knot is still adequate for most analysis needs and typically has less active users (i.e., a shorter job queue). 1.1 On campus Once your account has been created, the clusters can be accessed on campus via a terminal emulator using the ssh command or via a GUI SSH app, such as X2Go. On macOS or Linux the default terminal apps can be used. On Windows the PuTTY or X2Go apps can be used. Once a terminal is started, connect to Knot by entering the command $ ssh user@eddie.ecdf.ed.ac.uk where user is your server username. You will then be prompted for your password. SSH keys can also be set up to skip password entry. To use GUI applications on the server, a slightly different process is needed in order to enter an XWindows environment. On macOS, XQuartz must be installed. On Windows, use the X2Go app to connect to the server. More info on X2Go can be found at http://csc.cnsi.ucsb.edu/docs/using-x2go-gui-login-knot-or-pod. To use GUIs, add the -X option to the ssh command, such as $ ssh -X user@eddie.ecdf.ed.ac.uk Once connected, you will interact with the Linux server using the command line interface. The default working directory is your home folder /home/user. Note that this is the login node, but all analyses should be run on compute nodes. These analyses should be submitted as jobs to the queue (more about this later), which allocates compute nodes among users. To disconnect from the cluster, use the command exit. 1.2 Off campus The clusters can be accessed off campus using the VPN service for secure remote access. Instructions for installing the VPN app can be found at https://www.ed.ac.uk/information-services/computing/desktop-personal/vpn. Once the app is installed, open it and select connect for Remote Access. You will then be prompted to enter your userid and the associated password. Once connected, indicated by a green checkmark, you can then access a cluster as you normally would on campus by using a terminal or GUI SSH app. "],
["managing-files.html", "2 Managing files 2.1 Navigation 2.2 Viewing, creating, and editing files 2.3 Permissions 2.4 Shortcuts", " 2 Managing files Both Pod and Knot allow for 4 TB of file storage per user and more can be requested if needed. File management on the server clusters involves commonly used Linux commands to navigate and modify files and folders. The generic setup is $ command [options] &lt;arguments&gt; Most of the following commands have multiple options that are not covered here. For detailed documentation enter the command man &lt;command&gt;. Additionally, for a broader introduction to Linux commands see the Software Carpentry’s lessons at http://swcarpentry.github.io/shell-novice/. 2.1 Navigation pwd - present working directory cd dir - change directory (dir specifies directory path) cd - change to home directory (/home/user) cd .. - change to directory above current directory ls - list all files in current directory ls -a - list all files, including hidden files ls -l - list all files with permission information cp source destination - copy file mv source destination - move file mkdir name - make directory mkdir -m 770 name - make directory with permissions (770 or other number) rm file - remove file rmdir dir - remove empty directory (equivalent to rm -d dir) rm -r dir - remove non-empty directory shred -uz file - securely overwrite and delete file 2.2 Viewing, creating, and editing files file file.txt - view file type information stat file.txt - view file information more file.txt - view text file one screen at a time (to exit press q) less file.txt - view text file with scrolling (to exit press q) head file.csv - print first ten rows of text file tail file.csv - print last ten rows of text file head -1 file.csv - print column names if in first row of text file cut -d , -f 2,4-6 file.csv | less - view specific columns of CSV file by number position (e.g., 2,4-6) To view CSV files with pretty output, enter cat file.csv | sed -e 's/,,/, ,/g' | column -s, -t | less -#5 -N -S Note: This may not work well for all files. Use arrow keys to navigate and press q to exit. cat &gt; file.txt - create new text file (to save and exit press Ctrl+D) nano - create new text file using nano text editor nano file.txt - view and edit text file using nano text editor zip -r file.zip file1 folder1 - create compressed ZIP file with recursion unzip file.zip - extract ZIP file into working directory 2.3 Permissions Sometimes file and folder permissions need to be modified, such as to restrict access to files. On Linux, read, write, and execute permissions are represented by octal notation and applied to the file owner, groups, and all other users. # Permission rwx 7 read, write, and execute rwx 6 read and write rw- 5 read and execute r-x 4 read only r– 3 write and execute -wx 2 write only -w- 1 execute only –x 0 none — Common permissions include -rwxrwx--- = 770 - owner and group can do everything, but others can do nothing -rwxr-x--- = 750 - owner can do everything, group can read and execute only, but others can do nothing -rwx------ = 700 - owner can do everything, but group and others can do nothing To change permissions enter chmod 770 dir - change permissions for file or directory chmod -R 770 dir - change permissions recursively for directory chgrp group file - change group ownership for file or directory 2.4 Shortcuts Ctrl+A - move to beginning of line Ctrl+E - move to end of line Ctrl+U - clear line from cursor Ctrl+C - cancel command ~ - home directory . - current directory .. - one directory up * - wildcard completion Tab key - autocompletion Up and down arrow keys - cycle through command history "],
["transferring-files.html", "3 Transferring files 3.1 Using scp to transfer files 3.2 Using rsync to transfer files 3.3 Using rclone to transfer files", " 3 Transferring files To transfer files between a server and your local computer, use scp or rsync commands in the terminal or use a SFTP GUI app. On macOS, Cyberduck and FileZilla are good options. On Windows, WinSCP, Cyberduck, and FileZilla are good options. On Linux, FileZilla is a good option. Another option is to use Globus Online, a web app focused on large file transfers that allows pauses and breaks without loss of data. 3.1 Using scp to transfer files The scp command provides secure copying of files. For more detailed documentation enter the command man scp. The generic setup for an scp command is $ scp [options] &lt;source&gt; &lt;destination&gt; To copy a single file from a local computer to the Pod cluster, the scp command is $ scp /path/to/file.txt user@eddie.ecdf.ed.ac.uk:/home/user To copy an entire directory from a local computer to the Pod server cluster, the typical scp command is $ scp -r /path/to/project user@eddie.ecdf.ed.ac.uk:/home/user where the -r option indicates that files should be transferred recursively, such as subdirectories. 3.2 Using rsync to transfer files The rsync command line tool provides fast, incremental file transfer. The primary use case for rsync is to sync two folders, such as a synced backup folder. Compared to scp, rsync only transfers modified or new files and may also use partial transfers. As a result, rsync is typically faster than scp and SFTP, depending on the options used. For more detailed documentation enter the command man rsync or see https://rsync.samba.org/. On macOS and Linux, rsync is typically pre-installed. On Windows, you will need to install and use rsync via Cygwin. The generic setup for an rsync command is $ rsync [options] &lt;source&gt; &lt;destination&gt; To transfer an entire directory from a local computer to the Pod cluster, a typical rsync command with commonly used options is $ rsync -avzP -e ssh /home/user/project/ user@eddie.ecdf.ed.ac.uk:/home/user/project This example represents a push transfer. A pull transfer could also be completed by simply switching the source and destination. When transferring entire directories, a forward slash on the source matters but never for the destination. In this case, the ‘project’ folder has a forward slash and all its contents will be replicated exactly in the destination ‘project’ folder, copying all contents except the top ‘project’ folder. Without the forward slash on the source there would be another ‘project’ folder within the ‘project’ folder at the destination. The rsync options can be specified in short or long forms. In this use case, the -a or --archive option completely replicates all folders and files, including recursively through all subdirectories while preserving symbolic links, permissions, and ownership. The -v or --verbose option increases the amount of information that is logged. The -z or --compress option compresses files during transit to reduce transfer time. The -P or --partial --progress option enables partially transferred files to be kept in case of a break or pause and also displays the progress of individual file transfers. The -e ssh option instructs rysnc to transfer files via SSH, used when transferring to or from a server. Many other options exist, but these are the primary ones used. Additionally, add the -n or --dry-run option to test what an rsync command will do without actually transferring files. After the command is submitted, you will be prompted for a password if transferring to or from a server. If you use rsync frequently, then you can also set up SSH keys in order to skip password entry. For large transfers, sometimes the process needs to be paused and resumed, or sometimes the transfer can be interrupted due to a server, network, or power outage. The -P option should always be used to preserve the files or parts of files that have already been transferred and to avoid having to start over. To pause a transfer, use Ctrl+C. To resume, resubmit the same rsync command with the --append option added, which will restart the transfer where it left off. When syncing to a backup folder, add the --delete option to delete files and folders in the destination that have been deleted in the source. 3.3 Using rclone to transfer files The rclone command line tool transfers files to and from cloud storage services, like UCSB-provided unlimited Google Drive or Box storage. rclone is designed after rsync and its commands look similar. You will need to configure rclone so that it can access your cloud storage services. For detailed documentation see https://rclone.org/. "],
["submitting-jobs.html", "4 Submitting jobs 4.1 Knot 4.2 Pod", " 4 Submitting jobs The server clusters are shared resources among researchers, and a job queue process is used to manage and allocate resources among users. A job is simply a set of instructions that includes requests for resources and the specific set of commands—typically as scripts—to be executed, such as commands for transforming or analyzing data. When a user submits a job to the server for execution it enters the queue and is scheduled on a specific compute node for a specific time. 4.1 Knot The Knot cluster has 112 regular compute nodes with 12 cores per node and either 48 GB or 64 GB of RAM per node. There are also 4 ‘fat’ nodes with either 512 GB or 1 TB of RAM and 6 GPU nodes. Knot uses TORQUE PBS to schedule jobs. To submit a job, first create a new .pbs file with the nano editor using the command $ nano submit.pbs The typical structure of a .pbs file for a serial job using R is #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=2:00:00 #PBS -V cd $PBS_O_WORKDIR Rscript --vanilla script.R where 1 node with 1 processor is requested with a 2-hour timeframe for computation. The walltime option can be excluded if the computation time is unknown. The default walltime is 75 hours. The cd $PBS_O_WORKDIR changes the working directory to where the .pbs file is located. The Rscript --vanilla script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .pbs file is located or an absolute path can also be used. This line would change if using different software. There are also many other #PBS options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. Also be sure to include a blank line at the end of the file. To submit a job, use the command $ qsub submit.pbs For jobs that require less than one hour or are used for testing and debugging purposes, use the short queue to minimize waiting time with the command $ qsub -q short submit.pbs For jobs that require large memory, use the command $ qsub -q largemem submit.pbs for nodes with 256 GB/node or the command $ qsub -q xlargemem submit.pbs for nodes with 512 GB/node. The qsub command will return a job number. To check the status of a job, use the command $ qstat &lt;job number&gt; or $ qstat -u $USER To cancel or delete a job, use the command $ qdel &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .pbs file that was submitted, typically with the filename structure submit.pbs.[job number] unless otherwise specified. 4.2 Pod The Pod cluster is the newest on campus and offers the most compute resources. There are 64 regular compute nodes with 40 cores per node and 192 GB of RAM per node. There are also 4 ‘fat’ nodes with 1 TB of RAM per node and 3 GPU nodes. Pod uses SLURM to schedule jobs. To submit a job, first create a new .job file with the nano editor using the command $ nano submit.job The typical structure of a .job file for a serial job using R is #!/bin/bash -l #SBATCH --nodes=1 --ntasks-per-node=1 #SBATCH --time=2:00:00 cd $SLURM_SUBMIT_DIR module load R Rscript --vanilla script.R where 1 node with 1 processor is requested. The cd $PBS_O_WORKDIR changes the working directory to where the .job file is located. The module load R line loads R, and then the Rscript --vanilla script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .job file is located or an absolute path can also be used. These last two lines would change if using different software. There are also many other #SBATCH options that can be included. The default walltime for computation is 32 hours. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. Also be sure to include a blank line at the end of the file. To submit a job, use the command $ sbatch submit.job For jobs that require less than one hour or are used for testing and debugging purposes, you can use the short queue to minimize waiting time with the command $ sbatch -p short submit.job For jobs that require large memory, use the command $ sbatch -p largemem submit.job for nodes with 1 TB/node. The sbatch command will return a job number. To check the status of a job, use one of the commands $ squeue -j &lt;job number&gt; or $ squeue -u $USER To cancel or delete a job, use the command $ scancel &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .job file that was submitted, typically with the filename structure slurm-[jobnumber].out unless otherwise specified. "],
["using-private-modules.html", "5 Using Private Modules", " 5 Using Private Modules It would be slightly different from other HPC platform, when we want to use our own module, we will have to You will have three steps to go: - Install the software by making with binary files (note: ./configure --prefix=/home/path) - Make Private Module files inside the private module path $MODULEPATH - (export MODULEPATH=$HOME/privatemodules:$MODULEPATH) - Private Module file format could be found in the following example - Module load as ususal: `module load package/version_num ## Example of python 3.10 Install python 3.10 from official website (skip) Configure HPC (just do it once) vim .bashrc add export MODULEPATH=$HOME/privatemodules:$MODULEPATH source .bashrc Add ~/privatemodules/python/3.10 #%Module######################################################################## # # python 3.10.4 module file proc ModulesHelp { } { puts stderr &quot;\\tAdds python 3.10.4 to your environment&quot; } module-whatis &quot;Loads Python 3.10.4&quot; prepend-path PATH ~/src/bin prepend-path LD_LIBRARY_PATH ~/src/lib prepend-path C_INCLUDE_PATH ~/src/include prepend-path PKG_CONFIG_PATH ~/src/lib/pkgconfig prepend-path MANPATH ~/src/share/man prepend-path PYTHONPATH ~/src/lib/python3.10 Module load: module load python/3.10 Now it is really to go :) "],
["wiki-example.html", "6 Wiki + Example 6.1 Example: Using R", " 6 Wiki + Example From wiki, it provides a few guidance of using R, Tensorflow, Matlab, Java, Python, Singularity. 6.1 Example: Using R R is available on both Knot and Pod, including different versions of R. 6.1.1 Loading R To load R on Knot for interactive use, enter the command $ R To load R on Pod for interactive use, enter the commands $ load module R $ R To exit R, use the command q(). Note that the RStudio IDE is not available for use on the clusters. Remember: Most analyses should be performed on compute nodes by submitting batch jobs. The login node should only be used for simple analyses, testing, or debugging. 6.1.2 Versions On Knot, the available version of R is 3.2.2. On Pod, the available versions of R include R/3.2.2 R/3.4.4 R/3.5.1-multith R/3.5.1 The default is the latest version. To load a specific version, for example, use the command load module R/3.4.4. A different version of R can be installed inside your home directory. For more info see http://csc.cnsi.ucsb.edu/docs/using-r-knot-braid-and-pod. 6.1.3 Packages Packages can be installed using the R command install.packages(&quot;&lt;package&gt;&quot;) and should be stored inside your home folder. When using this command you may be prompted to select a CRAN mirror from which to download; select a USA (CA) mirror with an HTTPS connection for a fast and secure download. R should automatically add the home-based package library to .libPaths() (enter this command to confirm) and set it as default. 6.1.4 Example job files 6.1.4.1 Knot #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=2:00:00 #PBS -m abe #PBS -M user@ucsb.edu cd $PBS_O_WORKDIR Rscript --vanilla script.R 6.1.4.2 Pod #!/bin/bash -l #SBATCH --nodes=1 --ntasks-per-node=1 #SBATCH --time=2:00:00 #SBATCH --mail-type=ALL #SBATCH --mail-user=user@ucsb.edu cd $SLURM_SUBMIT_DIR module load R Rscript --vanilla script.R "],
["using-software-containers.html", "7 Using software containers", " 7 Using software containers Software containers can be used on the server clusters via Singularity. Containers enable fully reproducible research by packaging a computing environment and the necessary software packages and applications as a self-contained image file that can be easily transferred to and run on other systems. Containers are also useful for collaborating across different institutions and computing infrastructures. Find out more information at http://csc.cnsi.ucsb.edu/docs/containers. "],
["todo.html", "8 TODO", " 8 TODO This section is a quick guide of using GPU nodes. "]
]
